{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File \"AQM_Analyse\":\n",
    "1. Import the clean Data from SQL DB into Notebook\n",
    "2. Prepare Data for Analysis\n",
    "3. Create Dataframe to save the Results\n",
    "4. Create Loop, which executes multivariate OLS regressions using all combinations of 1 and 10 explanatory macroeconomic variables \n",
    "5. Save the Results from the Loop into the \"Result\"-Dataframe\n",
    "    For each Regression calculate:\n",
    "        Autocorrelation\n",
    "        heteroskedasticity\n",
    "        multicollinearity\n",
    "6. Export Results to SQL Lite into Table \"Results\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.6.0\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from math import exp, sqrt, log\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "import scipy\n",
    "import xlsxwriter\n",
    "\n",
    "from scipy import stats\n",
    "import pylab\n",
    "import statsmodels as sm\n",
    "from statsmodels.regression.linear_model import OLS\n",
    "import statsmodels.stats.api as sms\n",
    "from statsmodels.compat import lzip\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.stats.stattools import jarque_bera\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "import sqlite3\n",
    "from sqlite3 import Error\n",
    " \n",
    "\"\"\" create a database connection tool to a SQLite database \"\"\"\n",
    "def create_connection(db_file):\n",
    "    conn = None\n",
    "    try:\n",
    "        conn = sqlite3.connect(db_file)\n",
    "        print(sqlite3.version)\n",
    "    except Error as e:\n",
    "        print(e)\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    create_connection(r\"C:\\Users\\lione\\OneDrive\\Dokumente\\GitHub\\AQM\\SQL_DB.db\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Macro_Variable</th>\n",
       "      <th>CHCPIY_ECI</th>\n",
       "      <th>CHJOB_ECI</th>\n",
       "      <th>CHPMI_ECI</th>\n",
       "      <th>CNCPI_ECI</th>\n",
       "      <th>CNPMIB_ECI</th>\n",
       "      <th>CNURUA_ECI</th>\n",
       "      <th>EUHICY_ECI</th>\n",
       "      <th>EUPMI_ECI</th>\n",
       "      <th>EUUNR_ECI</th>\n",
       "      <th>RUCPIY_ECI</th>\n",
       "      <th>RUPMIM_ECI</th>\n",
       "      <th>RUUNR_ECI</th>\n",
       "      <th>USCPI_ECI</th>\n",
       "      <th>USPMI_ECI</th>\n",
       "      <th>USUNR_ECI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-31</th>\n",
       "      <td>-0.8</td>\n",
       "      <td>2.8</td>\n",
       "      <td>48.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>50.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>54.2</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-29</th>\n",
       "      <td>-0.9</td>\n",
       "      <td>2.8</td>\n",
       "      <td>48.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>51.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>53.3</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-31</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>50.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>53.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.2</td>\n",
       "      <td>3.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>53.5</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-30</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>46.5</td>\n",
       "      <td>3.4</td>\n",
       "      <td>53.3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.3</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>55.2</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-31</th>\n",
       "      <td>-1.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>45.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>50.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11.4</td>\n",
       "      <td>3.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>53.2</td>\n",
       "      <td>8.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Macro_Variable  CHCPIY_ECI  CHJOB_ECI  CHPMI_ECI  CNCPI_ECI  CNPMIB_ECI  \\\n",
       "DATE                                                                      \n",
       "2012-01-31            -0.8        2.8       48.0        4.5        50.5   \n",
       "2012-02-29            -0.9        2.8       48.2        3.2        51.0   \n",
       "2012-03-31            -1.0        2.8       50.6        3.6        53.1   \n",
       "2012-04-30            -1.0        2.8       46.5        3.4        53.3   \n",
       "2012-05-31            -1.0        2.9       45.8        3.0        50.4   \n",
       "\n",
       "Macro_Variable  CNURUA_ECI  EUHICY_ECI  EUPMI_ECI  EUUNR_ECI  RUCPIY_ECI  \\\n",
       "DATE                                                                       \n",
       "2012-01-31             NaN         2.7        NaN       10.9         4.2   \n",
       "2012-02-29             NaN         2.7        NaN       11.1         3.7   \n",
       "2012-03-31             NaN         2.7        NaN       11.2         3.7   \n",
       "2012-04-30             NaN         2.6        NaN       11.3         3.6   \n",
       "2012-05-31             NaN         2.4        NaN       11.4         3.6   \n",
       "\n",
       "Macro_Variable  RUPMIM_ECI  RUUNR_ECI  USCPI_ECI  USPMI_ECI  USUNR_ECI  \n",
       "DATE                                                                    \n",
       "2012-01-31             NaN        6.3        0.3       54.2        8.3  \n",
       "2012-02-29             NaN        6.2        0.2       53.3        8.3  \n",
       "2012-03-31             NaN        6.3        0.2       53.5        8.2  \n",
       "2012-04-30             NaN        5.6        0.2       55.2        8.2  \n",
       "2012-05-31             NaN        5.2       -0.2       53.2        8.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select Makro-Data from SQL DB: \n",
    "conn = sqlite3.connect('SQL_DB.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT * FROM MACRO_DATA\")\n",
    "new_data=c.fetchall()\n",
    "macro_data = pd.DataFrame(new_data,columns=['Macro_Variable', 'DATE', 'Value'])\n",
    "macro_data = macro_data.set_index('DATE')\n",
    "macro_data = macro_data.pivot(columns='Macro_Variable', values=['Value'])\n",
    "macro_data = macro_data.droplevel(level=0, axis=1)\n",
    "macro_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Instrument</th>\n",
       "      <th>CLc1</th>\n",
       "      <th>Cc1</th>\n",
       "      <th>GCc1</th>\n",
       "      <th>LCc1</th>\n",
       "      <th>NGLNMc1</th>\n",
       "      <th>NGc1</th>\n",
       "      <th>OJc1</th>\n",
       "      <th>PAc1</th>\n",
       "      <th>PLc1</th>\n",
       "      <th>SIc1</th>\n",
       "      <th>Wc1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2012-01-31</th>\n",
       "      <td>98.48</td>\n",
       "      <td>639.00</td>\n",
       "      <td>1737.8</td>\n",
       "      <td>124.85</td>\n",
       "      <td>56.25</td>\n",
       "      <td>2.503</td>\n",
       "      <td>210.00</td>\n",
       "      <td>685.95</td>\n",
       "      <td>1585.5</td>\n",
       "      <td>33.233</td>\n",
       "      <td>666.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-02-29</th>\n",
       "      <td>107.07</td>\n",
       "      <td>656.50</td>\n",
       "      <td>1709.9</td>\n",
       "      <td>127.65</td>\n",
       "      <td>59.38</td>\n",
       "      <td>2.616</td>\n",
       "      <td>190.55</td>\n",
       "      <td>706.65</td>\n",
       "      <td>1691.1</td>\n",
       "      <td>34.583</td>\n",
       "      <td>664.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-03-31</th>\n",
       "      <td>103.02</td>\n",
       "      <td>644.00</td>\n",
       "      <td>1669.3</td>\n",
       "      <td>120.45</td>\n",
       "      <td>61.50</td>\n",
       "      <td>2.126</td>\n",
       "      <td>164.50</td>\n",
       "      <td>653.15</td>\n",
       "      <td>1638.3</td>\n",
       "      <td>32.469</td>\n",
       "      <td>660.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-04-30</th>\n",
       "      <td>104.87</td>\n",
       "      <td>660.25</td>\n",
       "      <td>1663.4</td>\n",
       "      <td>118.75</td>\n",
       "      <td>57.87</td>\n",
       "      <td>2.285</td>\n",
       "      <td>140.85</td>\n",
       "      <td>681.75</td>\n",
       "      <td>1567.7</td>\n",
       "      <td>30.959</td>\n",
       "      <td>647.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2012-05-31</th>\n",
       "      <td>86.53</td>\n",
       "      <td>555.25</td>\n",
       "      <td>1562.6</td>\n",
       "      <td>117.10</td>\n",
       "      <td>53.98</td>\n",
       "      <td>2.422</td>\n",
       "      <td>112.05</td>\n",
       "      <td>612.10</td>\n",
       "      <td>1416.1</td>\n",
       "      <td>27.741</td>\n",
       "      <td>643.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Instrument    CLc1     Cc1    GCc1    LCc1  NGLNMc1   NGc1    OJc1    PAc1  \\\n",
       "DATE                                                                         \n",
       "2012-01-31   98.48  639.00  1737.8  124.85    56.25  2.503  210.00  685.95   \n",
       "2012-02-29  107.07  656.50  1709.9  127.65    59.38  2.616  190.55  706.65   \n",
       "2012-03-31  103.02  644.00  1669.3  120.45    61.50  2.126  164.50  653.15   \n",
       "2012-04-30  104.87  660.25  1663.4  118.75    57.87  2.285  140.85  681.75   \n",
       "2012-05-31   86.53  555.25  1562.6  117.10    53.98  2.422  112.05  612.10   \n",
       "\n",
       "Instrument    PLc1    SIc1     Wc1  \n",
       "DATE                                \n",
       "2012-01-31  1585.5  33.233  666.00  \n",
       "2012-02-29  1691.1  34.583  664.25  \n",
       "2012-03-31  1638.3  32.469  660.75  \n",
       "2012-04-30  1567.7  30.959  647.75  \n",
       "2012-05-31  1416.1  27.741  643.75  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select Commodities Data from SQL DB:   \n",
    "conn = sqlite3.connect('SQL_DB.db')\n",
    "c = conn.cursor()\n",
    "c.execute(\"SELECT * FROM COMMODITIES_RETURNS\")\n",
    "new_data=c.fetchall()\n",
    "ref_data = pd.DataFrame(new_data,columns=['DATE', 'CLOSE', 'Instrument', 'Return'])\n",
    "ref_data = ref_data.set_index('DATE')\n",
    "Commodities = ref_data.pivot(columns='Instrument', values=['CLOSE'])\n",
    "Commodities = Commodities.droplevel(level=0, axis=1)\n",
    "Commodities.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Rohdaten von API\n",
    "2. Rohdaten kontrollieren / Data cleaning\n",
    "    - Excel vergleichen\n",
    "    - Describe\n",
    "    - Bilder erstellen\n",
    "3. in Log-Return / Veränderung umwandeln\n",
    "    - Makro Daten -> %-Veränderung\n",
    "    - Commodity -> Log Returns\n",
    "4. Log-Returns und Veränderung auf Stationarität testen\n",
    "    - Dickey Fuller Test / Unit Root\n",
    "5. Correlation -> ???\n",
    "6. VIF_Results -> ???\n",
    "7. OLS\n",
    "    - adjusted R-Squared\n",
    "    - F-Statistik\n",
    "    - Durbin Watson\n",
    "    - JB\n",
    "    - Heterodesckity: Breusch Pagan Test\n",
    "- Multicorrelatiny -> ???\n",
    "- Autocorrelation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dickey Fuller Test / Unit Root Test on Commodity Returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tickers_loop=\"WIKI/\" + data_4['ticker'][0:20] + \" - Open\"\n",
    "tickers_loop=Commodities\n",
    "tickers_loop_list=list(tickers_loop.iloc[:].values)\n",
    "var_list=tickers_loop_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lags in range(1,3):\n",
    "    print('Number of lags used:',lags)\n",
    "    print('ADF P-Val for Prices (Level)')\n",
    "    for i in var_list:\n",
    "        adf_library = adfuller(Commodities[i], maxlag=lags, regression='nc',autolag=None)\n",
    "        adf_library_d = adfuller(np.diff(Commodities[i]), maxlag=lags, regression='nc',autolag=None)\n",
    "\n",
    "        print(i,':',\"%.2f\" %  adf_library[1],)\n",
    "    print('____________________________________________','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lags in range(1,3):\n",
    "    print('Number of lags used:',lags)\n",
    "    print('ADF P-Val for Absolute Returns (1st Difference)')\n",
    "    for i in var_list:\n",
    "        adf_library = adfuller(data_3[i], maxlag=lags, regression='nc',autolag=None)\n",
    "        adf_library_d = adfuller(np.diff(data_3[i]), maxlag=lags, regression='nc',autolag=None)\n",
    "\n",
    "        print(i,':',\"%.2f\" %  adf_library_d[1])\n",
    "    print('____________________________________________','\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for VIF Calculation - added a Return-Statement, so that the Results get added in a DataFrame:\n",
    "def vif_cal(input_data, dependent_col):\n",
    "    x_vars=input_data.drop([dependent_col], axis=1)\n",
    "    xvar_names=x_vars.columns\n",
    "    ttt = pd.DataFrame()\n",
    "    for i in range(0,xvar_names.shape[0]):\n",
    "        y=x_vars[xvar_names[i]] \n",
    "        x=x_vars[xvar_names.drop(xvar_names[i])]\n",
    "        rsq=sm.OLS(formula=\"y~x\", data=x_vars,endog=y, exog=x).fit().rsquared  \n",
    "        vif=round(1/(1-rsq),2)\n",
    "\n",
    "        tt = pd.DataFrame()\n",
    "        tt['XVAR'] = [xvar_names[i]]\n",
    "        tt['VIF'] = [vif]\n",
    "        ttt = ttt.append(tt)\n",
    "    return ttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF Calc: Code for 1 column:\n",
    "input_columns = macro_data[['CHCPIY_ECI','CHJOB_ECI','CHPMI_ECI','CNCPI_ECI','CNPMIB_ECI','EUHICY_ECI','EUUNR_ECI','RUCPIY_ECI','RUUNR_ECI','USCPI_ECI','USPMI_ECI','USUNR_ECI']]\n",
    "xxx = vif_cal(input_data=input_columns, dependent_col='CHCPIY_ECI')\n",
    "xxx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VIF Calc: Code for all Columns - Loop:\n",
    "input_columns = macro_data[['CHCPIY_ECI','CHJOB_ECI','CHPMI_ECI','CNCPI_ECI','CNPMIB_ECI','EUHICY_ECI','EUUNR_ECI','RUCPIY_ECI','RUUNR_ECI','USCPI_ECI','USPMI_ECI','USUNR_ECI']]\n",
    "VIF_Results = pd.DataFrame()\n",
    "for col in range(0, len(input_columns.columns)):\n",
    "    vif = vif_cal(input_data=input_columns, dependent_col=input_columns.columns[col])\n",
    "    vif = pd.DataFrame(vif)\n",
    "    vif['dep_col'] = input_columns.columns[col]\n",
    "    VIF_Results = VIF_Results.append(vif)\n",
    "VIF_Results = VIF_Results.set_index(['dep_col']).pivot(columns='XVAR', values=['VIF'])\n",
    "VIF_Results = VIF_Results.droplevel(level=0, axis=1)\n",
    "VIF_Results.to_excel('VIF_Results.xlsx')\n",
    "VIF_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OLS\n",
    "import statsmodels.api as sm\n",
    "y = Commodities['CLc1']\n",
    "x = macro_data[['CHCPIY_ECI','CHJOB_ECI','CHPMI_ECI','CNCPI_ECI','CNPMIB_ECI','EUHICY_ECI','EUUNR_ECI','RUCPIY_ECI','RUUNR_ECI','USCPI_ECI','USPMI_ECI','USUNR_ECI']]\n",
    "x = sm.add_constant(x)\n",
    "model = sm.OLS(y,x, missing = 'drop') \n",
    "results = model.fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "#print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table to save all Results from OLS:\n",
    "import statsmodels.api as sm\n",
    "Results_OLS = pd.DataFrame()\n",
    "Top_results_OLS = pd.DataFrame()\n",
    "VIF_Results = pd.DataFrame()\n",
    "Breusch_Pagan = pd.DataFrame()\n",
    "\n",
    "len_comm_col = len(Commodities.columns) \n",
    "for col in range(0, len_comm_col):\n",
    "    \n",
    "    #OLS Simulation for each Commodity:\n",
    "    y = Commodities[Commodities.columns[col]]\n",
    "    x = macro_data[['CHCPIY_ECI','CHJOB_ECI','CHPMI_ECI','CNCPI_ECI','CNPMIB_ECI','EUHICY_ECI','EUUNR_ECI','RUCPIY_ECI','RUUNR_ECI','USCPI_ECI','USPMI_ECI','USUNR_ECI']]\n",
    "    x = sm.add_constant(x)\n",
    "    model = sm.OLS(y,x, missing = 'drop') \n",
    "    results = model.fit(cov_type='HAC', cov_kwds={'maxlags':1})\n",
    "    print(results.summary())\n",
    "\n",
    "    # Breush Pagan Test for Heteroskedastic residuals\n",
    "    regression1 = pd.DataFrame(y)\n",
    "    prediction = results.predict()\n",
    "    regression1['Prediction'] = prediction\n",
    "    residuals1 = y - prediction\n",
    "    results_summary  = results.summary()\n",
    "    #plt.plot(residuals1)\n",
    "    #plt.title('Residuals')\n",
    "    #plt.show\n",
    "\n",
    "    pagan = pd.DataFrame()\n",
    "    name = ['Lagrange multiplier statistic', 'p-value', 'f-value', 'f p-value']\n",
    "    pagan = sms.het_breuschpagan(residuals1, results.model.exog)\n",
    "    breusch = lzip(name, pagan)\n",
    "    breusch = pd.DataFrame(breusch)\n",
    "    breusch['Commodity'] = Commodities.columns[col]\n",
    "    Breusch_Pagan = Breusch_Pagan.append(breusch)\n",
    "    Breusch_Pagan\n",
    "\n",
    "    #Append Results to the Results-Table:\n",
    "    results_as_html = results_summary.tables[1].as_html()\n",
    "    results = pd.read_html(results_as_html, header=0, index_col=0)[0]\n",
    "    results['Commodity'] = Commodities.columns[col]\n",
    "    Results_OLS = Results_OLS.append(results)\n",
    "    dfs = {}\n",
    "    fs = results_summary\n",
    "    for item in fs.tables[0].data:\n",
    "        dfs[item[0].strip()] = item[1].strip()\n",
    "        dfs[item[2].strip()] = item[3].strip()\n",
    "    for item in fs.tables[2].data:\n",
    "        dfs[item[0].strip()] = item[1].strip()\n",
    "        dfs[item[2].strip()] = item[3].strip()\n",
    "    dfs = pd.Series(dfs)\n",
    "    dfs = pd.DataFrame(dfs)\n",
    "    dfs.reset_index()\n",
    "    dfs['Commodity'] = Commodities.columns[col]\n",
    "    Top_results_OLS = Top_results_OLS.append(dfs)\n",
    "    Top_results_OLS\n",
    "\n",
    "Results_OLS.to_excel('OLS_Results_Bottom.xlsx')\n",
    "Top_results_OLS = Top_results_OLS.pivot(columns='Commodity', values=[0])\n",
    "Top_results_OLS = Top_results_OLS.droplevel(level=0, axis=1)\n",
    "Top_results_OLS.to_excel('OLS_Results_Top.xlsx')\n",
    "Breusch_Pagan = Breusch_Pagan.rename(columns={0:'Variable', 1:'Value'})\n",
    "Breusch_Pagan = Breusch_Pagan.set_index('Variable')\n",
    "Breusch_Pagan = Breusch_Pagan.pivot(columns='Commodity', values=['Value'])\n",
    "Breusch_Pagan = Breusch_Pagan.droplevel(level=0, axis=1)\n",
    "Breusch_Pagan.to_excel('Breusch_Pagan_Results.xlsx')\n",
    "\n",
    "#print Correlation Matrix -> braucht kein Loop, da überall immer das Gleiche:\n",
    "corr = macro_data[['CHCPIY_ECI','CHJOB_ECI','CHPMI_ECI','CNCPI_ECI','CNPMIB_ECI','EUHICY_ECI','EUUNR_ECI','RUCPIY_ECI','RUUNR_ECI','USCPI_ECI','USPMI_ECI','USUNR_ECI']].corr()\n",
    "corr.to_excel('Correlation_Matrix.xlsx')\n",
    "sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Results_OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Top_results_OLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diverse Code-Snippets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jarq Bera Test:\n",
    "stats.jarque_bera(ReturnS)\n",
    "\n",
    "#LjungBox autocorrelation:\n",
    "PF_Ljung = sm.stats.acorr_ljungbox(PF_Returns, lags=10)\n",
    "\n",
    "#Autocorrelation als Grafik:\n",
    "data = np.array(PF_Returns)\n",
    "plt.acorr(data, maxlags=20)\n",
    "plt.title('Autokorrelation der Portfolio-Renditen')\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Autokorrelation')\n",
    "plt.savefig('Autocorrelation.png', dpi=800)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#ad Fuller\n",
    "ts.adfuller(df['GDP'])\n",
    "\n",
    "\n",
    "\n",
    "#Breusch Pagan Test:\n",
    "statsmodels.stats.diagnostic.het_breuschpagan(results.resid, exog_het = x)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.dot(var1,var2) vector multiplication\n",
    "\n",
    "var1.T transposes the vector\n",
    "\n",
    "np.linalg.inv (var1) inverts the matrix\n",
    "\n",
    "np.diag (var1) extract a diagonal\n",
    "\n",
    "Def fct (var1,var2) defines the function ,\n",
    "\n",
    "OLS_results = library for displaying further\n",
    "\n",
    "Return OLS_results\n",
    "\n",
    "\n",
    "df = pd.DataFrame(np.random.randint(low=0, high=10, size=(5, 5)), \n",
    "                  columns=['Historic_Rate', 'Overnight', '1M', '3M', '6M'])\n",
    "\n",
    "fit_d = {}  # This will hold all of the fit results and summaries\n",
    "for col in [x for x in df.columns if x != 'Historic_Rate']:\n",
    "    Y = df['Historic_Rate'] - df['Historic_Rate'].shift(1)\n",
    "    # Need to remove the NaN for fit\n",
    "    Y = Y[Y.notnull()]\n",
    "\n",
    "    X = df[col] - df[col].shift(1)\n",
    "    X = X[X.notnull()]\n",
    "\n",
    "    X = sm.add_constant(X)  # Add a constant to the fit\n",
    "\n",
    "    fit_d[col] = sm.OLS(Y,X).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beispiel Loops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------------------------------------\n",
    "#-----------------------------------------------------------------------\n",
    "#Parameter definieren:\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "Anzahl_Simulationen = 1000 #1000, 2000, 3000 Simulationen eingeben\n",
    "Sample_length = 750 #Sample-Grösse für Bootstrap: 250, 500, 750, 1000\n",
    "alpha = 0.05 #Konfidenzintervall definieren 0.05 und 0.25\n",
    "Position_VaR = int(Anzahl_Simulationen * alpha) \n",
    "#Da unterschiedliche Anzahl Simulationen, dynamisch 5%-Position \n",
    "Anzahl_Wiederholungen = 5   \n",
    "#Konsistenz der Berechnung überprüfen, indem alles 5-Mal durchgeführt wird.\n",
    "\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "#Sample-Länge definieren:\n",
    "#Automatische Adjustierung der Sample-Länge:\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "Returns2 = Returns.reset_index()\n",
    "Sample_for_Bootstrap = Returns2[['Novartis', 'LafargeHolcim',\n",
    "                                 'Logitech', 'Nestle', 'Swiss_Re']]\n",
    "\n",
    "sample_length_adjustierung = (round((len(Sample_for_Bootstrap)-Sample_length)/(10)))*10 + Sample_length\n",
    "Returns_for_sample = Returns[:sample_length_adjustierung].reset_index()\n",
    "Sample_for_Bootstrap = Returns_for_sample[['Novartis', 'LafargeHolcim',\n",
    "                                           'Logitech', 'Nestle', 'Swiss_Re']]\n",
    "\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "#Loop 1: Alles 5 Mal ausführen und als Excel & Grafik speichern:\n",
    "#---------------------------------------------------------------------------------------------------------------\n",
    "for y in range(0,Anzahl_Wiederholungen):\n",
    "    BHS = [] #Tabelle mit allen berechneten VaR der BHS-Methode\n",
    "    CBB = [] #Tabelle mit allen berechneten VaR der CBB-Methode\n",
    "    Reality = [] #Tabelle für die jeweiligen 10 Tage, welche für das Backtesting benötigt werden\n",
    "    \n",
    "    #Dynamisch die Dateinamen für das Excel und der Grafik generieren:\n",
    "    excel_name = \"Simulationen/VaR_Simulation_\"+str(alpha)+\"_Sim_\"+str(Anzahl_Simulationen)+\"_length_\"+str(Sample_length)+\"_V\"+str(y+1)+ \".xlsx\"\n",
    "    Bild_name = \"Simulationen/VaR_Simulation_\"+str(alpha)+\"_Sim_\"+str(Anzahl_Simulationen)+\"_length_\"+str(Sample_length)+\"_V\"+str(y+1)+\".png\"\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #Loop 2: Rolling-Window-Methode: 750 Tage für die Durchführung des Bootstrappings + 10 Tage für Backtesting:\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    for x in range(0, len(Sample_for_Bootstrap)-Sample_length, 10): \n",
    "        Samp = Sample_for_Bootstrap[x:x+Sample_length] \n",
    "\n",
    "        #Nächsten 10 Tagen für das Backtesting wählen, in diskrete Portfolio-Renditen umrechnen und speichern:\n",
    "        T = Sample_for_Bootstrap.iloc[x+Sample_length:x+Sample_length+10]\n",
    "        Real_Sample = T.sum()\n",
    "        diskrete_real = Math.e**(Real_Sample)-1 \n",
    "        diskrete_real = diskrete_real.mean()  \n",
    "        Reality.append(diskrete_real)\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #Loop 3: \n",
    "            #Bootstrap Historical Simulation & Circular Bootstrap anwenden\n",
    "            #mit n Simulationen & zeitliche Addition der Log-Returns \n",
    "            #umrechnen in diskrete Returns und berechnung Mittelwert für Portfolio-Rendite:\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #Bootstrap Historical Simulation (BHS): 10 Tage nach Zufall mit Zurücklegen ziehen und dies 1000 mal wiederholen:\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        BHS_VaR = []\n",
    "        for x in range(0,Anzahl_Simulationen):\n",
    "            Stichproben = Samp.sample(n=10, replace=True).sum()\n",
    "            df2 = Math.e**(Stichproben)-1 \n",
    "            x = df2.mean() \n",
    "            BHS_VaR.append([x])   \n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #Circular Block Bootstrap (CBB): 10-Tages-Blöcke mit Circular-Effekt ziehen und dies 1000 mal wiederholen:\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        CBB_VaR = [] \n",
    "        for x in range(0,Anzahl_Simulationen):    \n",
    "            Random_X_from = np.random.randint(0,Sample_length+1)\n",
    "            Random_X_to = Random_X_from + 10\n",
    "            Stichproben_CBB = Samp[Random_X_from:Random_X_to]\n",
    "            \n",
    "            #-------------------------------------------------------------------------\n",
    "            #Circular-Effekt: Das Ende des Samples mit dem Anfang verbinden -> über den Rand\n",
    "            #-------------------------------------------------------------------------\n",
    "            if Random_X_to > Sample_length: #Überprüfen, ob Block über den Rand geht.\n",
    "                Random_X_to_Circle = (Random_X_to-Sample_length) #Anzahl fehlende Tage berechnen\n",
    "                Rest_Circular = Samp[:Random_X_to_Circle] #Fehlende Tage vom Anfang des Samples holen\n",
    "                Stichproben_CBB = pd.concat([Stichproben_CBB, Rest_Circular]) #Beide Listen zusammenfügen\n",
    "\n",
    "            Stichproben_CBB.sum()\n",
    "            Stichproben_CBB = Stichproben_CBB.sum()\n",
    "            df_CBB = Math.e**(Stichproben_CBB)-1  #Log Renditen in Diskrete Renditen umrechnen\n",
    "            x_CBB = df_CBB.mean()    #Portfolio Rendite rechnen\n",
    "            CBB_VaR.append([x_CBB])    #Tabelle mit 1000 Portfolio Renditen\n",
    "\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        #Alle 1000 simulierten Portfolio-Renditen aufsteigend sortieren und der 51 schlechteste als VaR definieren:\n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        CBB_VaR.sort()\n",
    "        PF_Returns_Simulation_CBB = pd.DataFrame(CBB_VaR)\n",
    "        VaR_CBB = PF_Returns_Simulation_CBB.iloc[Position_VaR]\n",
    "        CBB.append(VaR_CBB)\n",
    "\n",
    "        BHS_VaR.sort()\n",
    "        PF_Returns_Simulation_BHS = pd.DataFrame(BHS_VaR)\n",
    "        a = PF_Returns_Simulation_BHS.iloc[Position_VaR]\n",
    "        BHS.append(a)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #Die berechneten VaRs der beiden Methoden in neue Tabelle speichern & die tatsächliche 10-Tages-Rendite ergänzen:\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    Simulationen = pd.DataFrame(BHS)\n",
    "    Simulationen = Simulationen.rename(columns={0:'BHS_VaR'})\n",
    "    Simulationen['CBB_VaR'] = pd.DataFrame(CBB)\n",
    "    Simulationen = Simulationen.reset_index()\n",
    "    Simulationen.drop('index', axis=1, inplace=True)\n",
    "    Simulationen = pd.concat([Simulationen, pd.DataFrame(Reality)], axis=1)\n",
    "    Simulationen = Simulationen.rename(columns={0:'Reality'})\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #Grafik speichern:\n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    Simulationen.plot(figsize=(30,15));\n",
    "    plt.savefig(Bild_name, dpi=400)\n",
    "    \n",
    "    #---------------------------------------------------------------------------------------------------------------\n",
    "    #Anzahl Exceptions & Exceptions Rate berechnen:\n",
    "    #---------------------------------------------------------------------------------------------------------------  \n",
    "    Simulationen['BHS_Exceptions'] = Simulationen.BHS_VaR > Simulationen.Reality\n",
    "    Simulationen['Anzahl_BHS_Exceptions'] = (Simulationen.BHS_VaR > Simulationen['Reality']).sum()\n",
    "    Simulationen['BHS_Exception_Rate'] = Simulationen.Anzahl_BHS_Exceptions/(len(Simulationen))\n",
    "    \n",
    "    Simulationen['CBB_Exceptions'] = Simulationen.CBB_VaR > Simulationen.Reality\n",
    "    Simulationen['Anzahl_CBB_Exceptions'] = (Simulationen.CBB_VaR > Simulationen['Reality']).sum()\n",
    "    Simulationen['CBB_Exception_Rate'] = Simulationen.Anzahl_CBB_Exceptions/(len(Simulationen))\n",
    "    \n",
    "    Simulationen.to_excel(excel_name)\n",
    "    Simulationen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------\n",
    "#Abgelegte Excels für das Backtesting einlesen:\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "Sim = [1000, 2000, 3000]\n",
    "LL = [250, 500, 750, 1000]\n",
    "Anzahl_Wiederholungen = 5\n",
    "path = r'C:\\Users\\lione\\OneDrive\\ZHAW\\Bachelor-Arbeit\\Simulationen'\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "#Parameter eingeben: \n",
    "#-----------------------------------------------------------------------------------------------\n",
    "Hypo_alpha = 0.05  #Konfidenzniveau für Hypothesen-Test eingeben !!!!!!!!!!!! 0.25????\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "#Abgelegte Excels für das Backtesting mittels Schlaufen einlesen:\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "Results = pd.DataFrame()\n",
    "\n",
    "for S in Sim:\n",
    "    for L in LL:\n",
    "        for y in range(0,Anzahl_Wiederholungen):\n",
    "            \n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            #Alle Excel-Files importieren und als DataFrames speichern:\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            excel_name = \"\\VaR_Simulation_Sim_\"+str(S)+\"_length_\"+str(L)+\"_V\"+str(y+1)+\".xlsx\" #VaR 95%\n",
    "            import_name = path + excel_name\n",
    "            name = \"S_\"+str(S)+\"_L_\"+str(L)+\"_V\"+str(y+1)\n",
    "            vars()[name] = pd.read_excel(import_name, sep=';', index_col=0).sort_index()\n",
    "            \n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            #RUNS-TEST:\n",
    "            #Für jedes File den Runs-Test durchführen und den Z-Score und P-Wert speichern:\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            vars()[name]['RT_Z_Score_BHS'] = runstest_1samp(vars()[name].BHS_Exceptions, correction=False)[0]\n",
    "            vars()[name]['RT_P_Value_BHS'] = runstest_1samp(vars()[name].BHS_Exceptions, correction=False)[1]\n",
    "            vars()[name]['RT_reject_H0_BHS'] = vars()[name]['RT_P_Value_BHS'] <= Hypo_alpha\n",
    "            \n",
    "            vars()[name]['RT_Z_Score_CBB'] = runstest_1samp(vars()[name].CBB_Exceptions, correction=False)[0]\n",
    "            vars()[name]['RT_P_Value_CBB'] = runstest_1samp(vars()[name].CBB_Exceptions, correction=False)[1]\n",
    "            vars()[name]['RT_reject_H0_CBB'] = vars()[name]['RT_P_Value_CBB'] <= Hypo_alpha\n",
    "            vars()[name]['Run'] = name\n",
    "            \n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            #Erste Zeile jedes DataFrames wählen und in neue Tabelle mit allen Resultaten speichern:\n",
    "            #-----------------------------------------------------------------------------------------------\n",
    "            First_col = pd.DataFrame(vars()[name].iloc[0])\n",
    "            First_col = First_col.T\n",
    "            Results = pd.concat([Results, First_col])\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "#Tabelle mit den Resultaten optimieren und als Excel exportieren:\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "Results = Results.set_index('Run')\n",
    "Results = Results.drop(['BHS_VaR','CBB_VaR','Reality','BHS_Exceptions', 'CBB_Exceptions'], axis=1)\n",
    "Results = Results.rename(columns= \n",
    "                             {'Anzahl_BHS_Exceptions' : 'BHS_Exceptions'\n",
    "                             ,'Anzahl_CBB_Exceptions' : 'CBB_Exceptions'})\n",
    "Results['Anzahl_Tests'] = Results.BHS_Exceptions / Results.BHS_Exception_Rate\n",
    "Results.to_excel('Results_95.xlsx')\n",
    "Results_95 = Results \n",
    "Results_95"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
